{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b92948c5",
   "metadata": {},
   "source": [
    "## Regresión\n",
    "\n",
    "Para el problema de pronóstico, elegimos el dataset de [Air Pollution Forecasting](https://www.kaggle.com/datasets/rupakroy/lstm-datasets-multivariate-univariate?resource=download)\n",
    "\n",
    "Este conjunto de datos informa sobre el clima y el nivel de contaminación cada hora durante cinco años en la embajada de Estados Unidos en Pekín, China.\n",
    "\n",
    "Los datos incluyen la fecha y la hora, la concentración de PM2.5 y la información meteorológica, como el punto de rocío, la temperatura, la presión, la dirección y la velocidad del viento, y el número acumulado de horas de nieve y lluvia. El objetivo es predecir el nivel de contaminación de aire, que es el PM2.5\n",
    "\n",
    "La lista completa de características de los datos es la siguiente:\n",
    "\n",
    "- `date`: La fecha del registro (es por hora)\n",
    "- `pollution`: concentración de PM2.5 (variable objetivo)\n",
    "- `dew`: punto de rocío\n",
    "- `temp`: temperatura\n",
    "- `press`: presión\n",
    "- `wnd_dir`: dirección del viento combinada\n",
    "- `wnd_spd`: velocidad del viento acumulada\n",
    "- `snow`: horas de nieve acumuladas\n",
    "- `rain`: horas de lluvia acumuladas\n",
    "\n",
    "Nuestro flujo de trabajo será el siguiente:\n",
    "+ EDA\n",
    "+ Entrenamiento y selección de un modelo de pronóstico lineal clásico para el caso univariado\n",
    "+ Evaluación del modelo clásico\n",
    "+ Selección, entrenamiento y validación de distintos modelos basados en redes neuronales en el caso univariado\n",
    "+ Selección, entrenamiento y validación de distintos modelos basados en redes neuronales en el caso multivariado\n",
    "+ Comparación de los modelos de redes neuronales univariados vs los multivariados\n",
    "+ Comparación del modelo clásico univariado contra las redes neuronales univariadas\n",
    "+ Selección del mejor modelo general y optimizarlo con `Optuna`\n",
    "\n",
    "Para el registro y comparación de modelos usaremos MLFlow"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0aded474",
   "metadata": {},
   "source": [
    "#### Cargar los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9e4d294",
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mLa ejecución de celdas con '.venv (Python 3.11)' requiere el paquete ipykernel.\n",
      "\u001b[1;31mEjecute el siguiente comando para instalar 'ipykernel' en el entorno de Python. comando \n",
      "\u001b[1;31m: '/Users/erick/Library/CloudStorage/OneDrive-ITESO/Modelos/proyecto-modelos/.venv/bin/python -m pip install ipykernel -U --force-reinstall'"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from sklearn.preprocessing import StandardScaler, LabelEncoder\n",
    "from statsmodels.graphics.tsaplots import plot_acf, plot_pacf\n",
    "from statsmodels.tsa.stattools import adfuller\n",
    "from statsmodels.tsa.statespace.sarimax import SARIMAX\n",
    "import statsmodels.api as sm\n",
    "from scipy.stats import boxcox\n",
    "import statsmodels as st\n",
    "from statsmodels.tsa.seasonal import MSTL\n",
    "from sklearn.preprocessing import power_transform\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from scipy.stats import normaltest\n",
    "from typing import List, Optional, Tuple\n",
    "import itertools\n",
    "from statsmodels.stats.diagnostic import acorr_ljungbox\n",
    "from scipy.special import inv_boxcox\n",
    "import scipy.stats as stats\n",
    "import dagshub\n",
    "import mlflow\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_percentage_error, r2_score\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "c539de79",
   "metadata": {},
   "outputs": [],
   "source": [
    "class LinearForecast:\n",
    "    def __init__(self, data: Optional[pd.DataFrame] = None):\n",
    "        self.data = data\n",
    "        self.results = None  # Holds the SARIMAX fitted model if needed\n",
    "\n",
    "    def read_data(self, data_path: str = None, data: pd.DataFrame = None, y_col: str = \"y\", date_col: str = \"date\"):\n",
    "        \"\"\"\n",
    "        Reads data from an Excel file or directly from a DataFrame, sets the date column as index, and renames the target column to 'y'.\n",
    "\n",
    "        Parameters:\n",
    "        - data_path (str): The path to the Excel file (optional if data is provided).\n",
    "        - data (pd.DataFrame): Pre-loaded data as a DataFrame (optional if data_path is provided).\n",
    "        - y_col (str): The name of the target column to rename to 'y'.\n",
    "        - date_col (str): The name of the column to set as the index.\n",
    "\n",
    "        Returns:\n",
    "        - pd.DataFrame: The processed DataFrame.\n",
    "        \"\"\"\n",
    "        if data is not None:\n",
    "            self.data = data\n",
    "        elif data_path is not None:\n",
    "            self.data = pd.read_excel(data_path)\n",
    "        else:\n",
    "            raise ValueError(\"Either data_path or data must be provided.\")\n",
    "\n",
    "        self.data = self.data.set_index(date_col)\n",
    "        self.data = self.data.rename(columns={y_col: \"y\"})\n",
    "        return self.data\n",
    "\n",
    "\n",
    "    def plot_acf_pacf(self, data=None, kwargs=None):\n",
    "        \"\"\"\n",
    "        Plots the ACF and PACF of a given time series.\n",
    "        If no data is provided, it uses the 'y' column from self.data.\n",
    "        \"\"\"\n",
    "        if kwargs is None:\n",
    "            kwargs = {}\n",
    "\n",
    "        data = data if data is not None else self.data[\"y\"]\n",
    "\n",
    "        f = plt.figure(figsize=(8, 5))\n",
    "        ax1 = f.add_subplot(121)\n",
    "        plot_acf(data, zero=False, ax=ax1, **kwargs)\n",
    "\n",
    "        ax2 = f.add_subplot(122)\n",
    "        plot_pacf(data, zero=False, ax=ax2, method=\"ols\", **kwargs)\n",
    "        plt.show()\n",
    "\n",
    "    def adf_test(self, timeseries=None):\n",
    "        \"\"\"\n",
    "        Performs the Augmented Dickey-Fuller test to check stationarity of a given time series.\n",
    "        If no timeseries is provided, it uses self.data['y'].\n",
    "        \"\"\"\n",
    "        timeseries = timeseries if timeseries is not None else self.data[\"y\"]\n",
    "\n",
    "        print(\"Results of Dickey-Fuller Test:\")\n",
    "        dftest = adfuller(timeseries, autolag=\"AIC\")\n",
    "        dfoutput = pd.Series(dftest[0:4],\n",
    "                             index=[\"Test Statistic\", \"p-value\", \"#Lags Used\", \"Number of Observations Used\"])\n",
    "\n",
    "        for key, value in dftest[4].items():\n",
    "            dfoutput[\"Critical Value (%s)\" % key] = value\n",
    "        print(dfoutput)\n",
    "\n",
    "        if (dftest[1] <= 0.05) & (dftest[4][\"5%\"] > dftest[0]):\n",
    "            print(\"\\u001b[32mStationary\\u001b[0m\")\n",
    "        else:\n",
    "            print(\"\\x1b[31mNon-stationary\\x1b[0m\")\n",
    "\n",
    "    def mstl_descomposition(self, periods_seasonality, stl_kwargs=None):\n",
    "        \"\"\"\n",
    "        Performs an MSTL decomposition on the 'y' column of self.data.\n",
    "        Returns the decomposition results and also plots them.\n",
    "        \"\"\"\n",
    "        if stl_kwargs is None:\n",
    "            stl_kwargs = {\"seasonal_deg\": 0}\n",
    "\n",
    "        model = MSTL(self.data[\"y\"], periods=periods_seasonality, stl_kwargs=stl_kwargs)\n",
    "        res = model.fit()\n",
    "\n",
    "        fig, ax = plt.subplots(3 + len(periods_seasonality), 1, sharex=True, figsize=(8, 8))\n",
    "        res.observed.plot(ax=ax[0])\n",
    "        ax[0].set_ylabel(\"Observed\")\n",
    "        res.trend.plot(ax=ax[1])\n",
    "        ax[1].set_ylabel(\"Trend\")\n",
    "\n",
    "        for i, s_period in enumerate(periods_seasonality):\n",
    "            res.seasonal[f\"seasonal_{s_period}\"].plot(ax=ax[i + 2])\n",
    "            ax[i + 2].set_ylabel(f\"seasonal_{s_period}\")\n",
    "\n",
    "        res.resid.plot(ax=ax[2 + len(periods_seasonality)])\n",
    "        ax[2 + len(periods_seasonality)].set_ylabel(\"Residual\")\n",
    "\n",
    "        fig.tight_layout()\n",
    "        return res\n",
    "\n",
    "    def plot_time_series(self, figsize=(10, 5)):\n",
    "        \"\"\"\n",
    "        Plots the main 'y' time series.\n",
    "        \"\"\"\n",
    "        f = plt.figure(figsize=figsize)\n",
    "        ax = f.add_subplot(111)\n",
    "        ax.plot(self.data[\"y\"])\n",
    "        plt.title(\"Time Series Plot\")\n",
    "        plt.show()\n",
    "\n",
    "    def plot_boxp(self, data=None, figsize=(10, 5)):\n",
    "        \"\"\"\n",
    "        Plots a boxplot for the provided DataFrame or for self.data if none is provided.\n",
    "        Ensures the data is converted to a DataFrame and index is reset to avoid issues with time series indexing.\n",
    "        \"\"\"\n",
    "        if data is None:\n",
    "            data = self.data.copy()\n",
    "\n",
    "        # Convert to DataFrame if not already\n",
    "        data_box = pd.DataFrame(data).reset_index(drop=True)\n",
    "\n",
    "        f = plt.figure(figsize=figsize)\n",
    "        data_box.boxplot()\n",
    "        plt.title(\"Box Plot\")\n",
    "        plt.show()\n",
    "\n",
    "    def timeseries_transformation(self, transformation=\"log\"):\n",
    "        \"\"\"\n",
    "        Applies a specified transformation to the 'y' series and plots histograms and boxplots\n",
    "        for the original and transformed data. Supported transformations are 'log', 'sqrt', 'boxcox', 'yeo-johnson'.\n",
    "        Box-Cox transformation is skipped if values are not strictly positive.\n",
    "        \"\"\"\n",
    "        data = self.data[\"y\"]\n",
    "        FIG_SIZE_LARGE = (10, 5)\n",
    "\n",
    "        def plot_histograms(data_transformed, original_title, transformed_title):\n",
    "            fig, ax = plt.subplots(1, 2, figsize=FIG_SIZE_LARGE)\n",
    "            data.hist(bins=50, ax=ax[0])\n",
    "            ax[0].set_title(original_title)\n",
    "            data_transformed.hist(bins=50, ax=ax[1])\n",
    "            ax[1].set_title(transformed_title)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        def plot_boxplots(data_transformed, original_title, transformed_title):\n",
    "            fig, ax = plt.subplots(1, 2, figsize=FIG_SIZE_LARGE)\n",
    "            pd.DataFrame(data).boxplot(ax=ax[0])\n",
    "            ax[0].set_title(original_title)\n",
    "            pd.DataFrame(data_transformed).boxplot(ax=ax[1])\n",
    "            ax[1].set_title(transformed_title)\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        transformations = {\n",
    "            \"sqrt\": lambda d: np.sqrt(d),\n",
    "            \"log\": lambda d: np.log1p(d),\n",
    "            \"boxcox\": lambda d: pd.DataFrame({\n",
    "                \"box_cox\": power_transform(d.to_numpy().reshape(-1, 1), method=\"box-cox\").squeeze()\n",
    "            }) if (d > 0).all() else None,\n",
    "            \"yeo-johnson\": lambda d: pd.DataFrame({\n",
    "                \"yeo-johnson\": power_transform(d.to_numpy().reshape(-1, 1), method=\"yeo-johnson\").squeeze()\n",
    "            }),\n",
    "        }\n",
    "\n",
    "        if transformation not in transformations:\n",
    "            raise ValueError(f\"Unsupported transformation: {transformation}\")\n",
    "\n",
    "        if transformation == \"boxcox\" and not (data > 0).all():\n",
    "            print(\"Skipping Box-Cox transformation: Data must be strictly positive.\")\n",
    "            return\n",
    "\n",
    "        transformed = transformations[transformation](data)\n",
    "        if transformed is None:\n",
    "            print(f\"Transformation {transformation} could not be applied. Check data or transformation requirements.\")\n",
    "            return\n",
    "\n",
    "        plot_histograms(transformed, \"Original Data\", f\"Transformed ({transformation.title()})\")\n",
    "        plot_boxplots(transformed, \"Original Data\", f\"Transformed ({transformation.title()})\")\n",
    "\n",
    "\n",
    "    def timeseries_scaler(self, scaler=\"minmax\"):\n",
    "        \"\"\"\n",
    "        Scales the 'y' series using the specified scaler ('minmax' or 'standard').\n",
    "        Returns the scaled values as a numpy array.\n",
    "        \"\"\"\n",
    "        data = self.data[\"y\"]\n",
    "\n",
    "        if scaler == \"minmax\":\n",
    "            mm_scaler = MinMaxScaler()\n",
    "            scaled_values = mm_scaler.fit_transform(data.values.reshape(-1, 1))\n",
    "        elif scaler == \"standard\":\n",
    "            standard_scaler = StandardScaler()\n",
    "            scaled_values = standard_scaler.fit_transform(data.values.reshape(-1, 1))\n",
    "        else:\n",
    "            raise ValueError(f\"Scaler '{scaler}' is not valid. Choose from 'minmax' or 'standard'.\")\n",
    "\n",
    "        return scaled_values\n",
    "\n",
    "    def grid_search(self, p: list, d: list, q: list, P: list, D: list, Q: list, X: list, trends: list,\n",
    "                    start: int = 0, end: int = 10, perc: float = 0.1, data = None):\n",
    "        \"\"\"\n",
    "        Perform a grid search for SARIMAX hyperparameters over the provided lists of (p, d, q, P, D, Q, trends).\n",
    "        Only a portion (perc) of the data is used for training to speed up the search.\n",
    "        \"\"\"\n",
    "\n",
    "        if data is None:\n",
    "            data = self.data.copy()\n",
    "\n",
    "        train_data = self.data[:int(perc * len(self.data))]\n",
    "\n",
    "        no_estacional = list(itertools.product(p, d, q))\n",
    "        estacional = list(itertools.product(P, D, Q, X))\n",
    "\n",
    "        sarimax_params = list(itertools.product(no_estacional, estacional, trends))\n",
    "\n",
    "        resultados = pd.DataFrame(columns=[\"params\", \"AIC\", \"BIC\", \"LLF\"])\n",
    "\n",
    "        for i, (non_seasonal, seasonal, trend) in enumerate(sarimax_params[start:end]):\n",
    "            mod = SARIMAX(\n",
    "                endog=train_data,\n",
    "                trend=trend,\n",
    "                order=non_seasonal,\n",
    "                seasonal_order=seasonal,\n",
    "            )\n",
    "            results = mod.fit(disp=False)\n",
    "\n",
    "            resultados.loc[i, \"params\"] = str((non_seasonal, seasonal, trend))\n",
    "            resultados.loc[i, \"AIC\"] = results.aic\n",
    "            resultados.loc[i, \"BIC\"] = results.bic\n",
    "            resultados.loc[i, \"LLF\"] = results.llf\n",
    "\n",
    "        return resultados\n",
    "\n",
    "    def train_sarimax(self, sarimax_params: tuple, perc: float = 0.75, data=None):\n",
    "        \"\"\"\n",
    "        Train a SARIMAX model using specified parameters:\n",
    "        sarimax_params is a tuple (non_seasonal_order, seasonal_order, trend).\n",
    "        \"\"\"\n",
    "\n",
    "        if data is None:\n",
    "            data = self.data.copy()\n",
    "\n",
    "        n_train = int(perc * len(self.data))\n",
    "        train_data = self.data[:n_train]\n",
    "\n",
    "        mod = SARIMAX(\n",
    "            endog=train_data,\n",
    "            trend=sarimax_params[2],  # Trend ('n', 'c', 'ct', etc.)\n",
    "            order=sarimax_params[0],  # Non-seasonal order (p, d, q)\n",
    "            seasonal_order=sarimax_params[1],  # Seasonal order (P, D, Q, m)\n",
    "        )\n",
    "        self.results = mod.fit(disp=False)\n",
    "        print(f\"AIC={self.results.aic}\")\n",
    "        print(f\"BIC={self.results.bic}\")\n",
    "        print(f\"Log-likelihood={self.results.llf}\")\n",
    "        return self.results\n",
    "\n",
    "    def predict_sarimax(self, n_periods: int = 100, data = None, perc: float = 0.75):\n",
    "        \"\"\"\n",
    "        Predict values using the trained SARIMAX model for the desired number of periods.\n",
    "        The split is 75/25. The function returns a DataFrame with the predictions.\n",
    "        \"\"\"\n",
    "        if data is None:\n",
    "            data = self.data.copy()\n",
    "        if self.results is None:\n",
    "            raise ValueError(\"Model has not been trained yet. Please call train_sarimax first.\")\n",
    "\n",
    "        n_train = int(perc * len(self.data))\n",
    "        train_data = self.data[:n_train]\n",
    "        n_test = len(self.data) - n_train\n",
    "        test_data = self.data[-n_test:]\n",
    "\n",
    "        pred = self.results.predict(len(train_data), len(train_data) + len(test_data) - 1)\n",
    "        pred = pred[:n_periods].to_frame(name=\"prediction\")\n",
    "\n",
    "        pred.plot()\n",
    "        plt.legend([\"Prediction\"])\n",
    "        plt.title(\"SARIMAX Prediction\")\n",
    "        plt.show()\n",
    "        return pred\n",
    "\n",
    "    def forecast_sarimax(self, n_periods: int = 100, col_analysis: str = \"y\"):\n",
    "        \"\"\"\n",
    "        Forecast using the SARIMAX model and include confidence intervals.\n",
    "        Returns a DataFrame with actuals, confidence intervals, and predicted mean.\n",
    "        \"\"\"\n",
    "        if self.results is None:\n",
    "            raise ValueError(\"Model has not been trained yet. Please call train_sarimax first.\")\n",
    "\n",
    "        prediction = self.results.get_prediction()\n",
    "        pred = pd.concat([self.data, prediction.conf_int(), prediction.predicted_mean], axis=1)\n",
    "\n",
    "        fig, ax = plt.subplots()\n",
    "        pred[col_analysis][:n_periods].plot(style=\"r--\", label=\"Actual\", ax=ax)\n",
    "        pred[\"predicted_mean\"][:n_periods].plot(style=\"b\", alpha=0.4, label=\"Prediction\", ax=ax)\n",
    "        plt.legend()\n",
    "        plt.title(\"SARIMAX Forecast\")\n",
    "        plt.show()\n",
    "        return pred\n",
    "\n",
    "    def horizon_pred(self, horizon: int, num: int, col_analysis: str, order: tuple, seasonal_order: tuple):\n",
    "        \"\"\"\n",
    "        Perform a rolling horizon prediction for a specified time period.\n",
    "        horizon: number of steps to forecast\n",
    "        num: how many data points from the end of self.data to use (the last 'num' points)\n",
    "        col_analysis: column name for analysis (e.g., 'y')\n",
    "        order: non-seasonal order tuple (p, d, q)\n",
    "        seasonal_order: seasonal order tuple (P, D, Q, m)\n",
    "        \"\"\"\n",
    "        if self.data is None or self.data.empty:\n",
    "            raise ValueError(\"The data parameter is empty or None.\")\n",
    "        if col_analysis not in self.data.columns:\n",
    "            raise ValueError(f\"'{col_analysis}' column is missing in the input data.\")\n",
    "\n",
    "        y_train = self.data[col_analysis][-num:-horizon]\n",
    "        y_test = self.data[col_analysis][-horizon:]\n",
    "        y_pred = []\n",
    "\n",
    "        for _ in range(horizon):\n",
    "            mod = SARIMAX(\n",
    "                endog=y_train,\n",
    "                trend=\"ct\",\n",
    "                order=order,\n",
    "                seasonal_order=seasonal_order,\n",
    "            )\n",
    "            results = mod.fit(disp=False)\n",
    "            pred = results.predict(start=len(y_train), end=len(y_train))\n",
    "            y_train = np.append(y_train, pred[0])\n",
    "            y_pred.append(pred[0])\n",
    "\n",
    "        pred_df = pd.DataFrame({\"Actual\": y_test, \"Predicted\": y_pred})\n",
    "        pred_df.plot(title=\"Horizon Prediction\")\n",
    "        return pred_df\n",
    "\n",
    "    def error_metric(self, y_test, y_pred):\n",
    "        \"\"\"\n",
    "        Calculate evaluation metrics for predictions (MAPE, MAD).\n",
    "        \"\"\"\n",
    "        if len(y_test) != len(y_pred):\n",
    "            raise ValueError(\"Lengths of y_test and y_pred must match.\")\n",
    "\n",
    "        mape = np.mean(\n",
    "            np.abs(\n",
    "                (np.array(y_test) - np.array(y_pred)) / np.where(np.array(y_test) == 0, 1, np.array(y_test))\n",
    "            )\n",
    "        ) * 100\n",
    "        mad = np.mean(np.abs(np.array(y_test) - np.array(y_pred)))\n",
    "\n",
    "        return {\"MAPE\": mape, \"MAD\": mad}\n",
    "\n",
    "    def consolidated_ts_df(self, data: Optional[pd.DataFrame] = None, plot_graphs: bool = True) -> pd.DataFrame:\n",
    "        \"\"\"\n",
    "        Creates a DataFrame that consolidates multiple transformations and scalings of the 'y' series\n",
    "        (log, sqrt, boxcox, yeo-johnson, min-max scaler, standard scaler).\n",
    "        If no data is provided, it uses self.data.\n",
    "        Optionally plots histograms of all generated columns if plot_graphs is True.\n",
    "        Returns the DataFrame and the lambda value used for the Box-Cox transformation.\n",
    "        \"\"\"\n",
    "        if data is None:\n",
    "            df = self.data.copy()\n",
    "        else:\n",
    "            df = data.copy()\n",
    "\n",
    "        # Ensure 'y' is present in df\n",
    "        if 'y' not in df.columns:\n",
    "            raise ValueError(\"The dataframe must have a 'y' column for transformations.\")\n",
    "\n",
    "        # Apply transformations\n",
    "        df['log'] = np.log1p(df['y'])\n",
    "        df['sqrt'] = np.sqrt(df['y'])\n",
    "\n",
    "        # Box-Cox transformation (with lambda retrieval)\n",
    "        if (df['y'] > 0).all():  # Box-Cox requires strictly positive values\n",
    "            df['box_cox'], lambda_value = boxcox(df['y'])\n",
    "        else:\n",
    "            df['box_cox'] = np.nan  # Skip Box-Cox if data is not strictly positive\n",
    "            lambda_value = None\n",
    "\n",
    "        # Yeo-Johnson transformation\n",
    "        df['yeo_j'] = power_transform(df['y'].to_numpy().reshape(-1, 1), method='yeo-johnson')\n",
    "\n",
    "        # Min-Max scaling\n",
    "        df['mm_scaler'] = MinMaxScaler().fit_transform(df['y'].values.reshape(-1, 1))\n",
    "\n",
    "        # Standard scaling\n",
    "        df['std_scaler'] = StandardScaler().fit_transform(df['y'].values.reshape(-1, 1))\n",
    "\n",
    "        # Plot histograms\n",
    "        if plot_graphs:\n",
    "            df.hist(bins=50, figsize=(10, 8))\n",
    "            plt.tight_layout()\n",
    "            plt.show()\n",
    "\n",
    "        return df, lambda_value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52f8e18c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(\"data/Regression/LSTM-Multivariate_pollution.csv\")\n",
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41ef1204",
   "metadata": {},
   "source": [
    "### EDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9abc4a1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4243c0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def reporte(df):\n",
    "    dtyp = pd.DataFrame(df.dtypes, columns=['Tipo'])\n",
    "    missing = pd.DataFrame(df.isnull().sum(), columns=['Valores_Nulos'])\n",
    "    unival = pd.DataFrame(df.nunique(), columns=['Valores_Unicos'])\n",
    "    maximo = pd.DataFrame(df.max(), columns=['Max'])\n",
    "    minimo = pd.DataFrame(df.min(), columns=['Min'])\n",
    "    return dtyp.join(missing).join(unival).join(maximo).join(minimo)\n",
    "\n",
    "reporte(df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5cc0ff0d",
   "metadata": {},
   "source": [
    "Podemos ver que los datos recolectados son desde el 2do de enero de 2010 hasta el 31 de diciembre de 2014. No hay datos nulos en ninguna de nuestras variables. Vamos a convertir nuestra columna date a datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "1d3a0a9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['date'] = pd.to_datetime(df['date'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0216bc14",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Calculo de estadísticos básicos.\n",
    "# Algunas utilerías que vamos a usar\n",
    "def autolabel(rects, ax):\n",
    "    \"\"\"\n",
    "    Método auxiliar para agregarle el númerito correspondiente a su valor\n",
    "    a la barra en una gráfica de barras.\n",
    "\n",
    "    Esta función no la hice yo (aunque sí la modifiqué). La origi está en:\n",
    "    https://matplotlib.org/3.1.1/gallery/lines_bars_and_markers/barchart.html\n",
    "\n",
    "    rects: La figura de la gráfica guardada en una variable\n",
    "    ax: El eje donde se está graficando.\n",
    "    \"\"\"\n",
    "    # attach some text labels\n",
    "    for rect in rects:\n",
    "        height = rect.get_height()\n",
    "        ax.text(rect.get_x() + rect.get_width()/2.,\n",
    "                1.05*height,\n",
    "                '%d'%int(height),\n",
    "                ha='center', va='bottom')\n",
    "\n",
    "def _get_colors_to_use(variables):\n",
    "    \"\"\" Función para asignarle colores crecientes a una lista de elements\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    variables: Lista de elementos a los cuales les queremos asignar color\n",
    "\n",
    "\n",
    "    Regresa\n",
    "    -------\n",
    "    Dictionario de la forma: {element: color}\n",
    "    \"\"\"\n",
    "    colors = plt.cm.jet(np.linspace(0, 1, len(variables)))\n",
    "    return dict(zip(variables, colors))\n",
    "\n",
    "\n",
    "def plot_numeric(df, numeric_stats):\n",
    "\n",
    "    metrics = ['mean', 'median', 'std', 'q25', 'q75', 'nulls']\n",
    "    colors = _get_colors_to_use(metrics)\n",
    "\n",
    "    for index, variable in enumerate(sorted(numeric_stats.keys())):\n",
    "\n",
    "        # Plotting basic metrics\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=3, figsize=(10, 5))\n",
    "\n",
    "        bar_position = -1\n",
    "        for metric, value in numeric_stats[variable].items():\n",
    "            bar_position += 1\n",
    "\n",
    "            if value is None or np.isnan(value):\n",
    "                value = -1\n",
    "\n",
    "            # Plotting bars\n",
    "            bar_plot = ax[0].bar(bar_position, value,\n",
    "                                 label=metric, color=colors[metric])\n",
    "            autolabel(bar_plot, ax[0])\n",
    "\n",
    "            # Plotting histogram\n",
    "            df[variable].plot(kind='hist', color='blue',\n",
    "                              alpha=0.4, ax=ax[1])\n",
    "\n",
    "            # Plotting boxplot\n",
    "            df.boxplot(ax=ax[2], column=variable)\n",
    "\n",
    "            ax[0].set_xticks(range(len(metrics)))\n",
    "            ax[0].set_xticklabels(metrics, rotation=90)\n",
    "            ax[2].set_xticklabels('', rotation=90)\n",
    "\n",
    "            ax[0].set_title('\\n Basic metrics \\n', fontsize=10)\n",
    "            ax[1].set_title('\\n Data histogram \\n', fontsize=10)\n",
    "            ax[2].set_title('\\n Data boxplot \\n', fontsize=10)\n",
    "            fig.suptitle(f'Variable: {variable} \\n\\n\\n', fontsize=15)\n",
    "\n",
    "            fig.tight_layout()\n",
    "    return\n",
    "\n",
    "\n",
    "def plot_categorical(df, object_stats):\n",
    "\n",
    "    metrics = ['unique_vals', 'mode', 'null_count']\n",
    "    colors = _get_colors_to_use(metrics)\n",
    "\n",
    "    for index, variable in enumerate(sorted(object_stats.keys())):\n",
    "\n",
    "        # Plotting basic metrics\n",
    "        fig, ax = plt.subplots(nrows=1, ncols=1, figsize=(10, 5))\n",
    "\n",
    "        bar_position = -1\n",
    "        for metric, value in object_stats[variable].items():\n",
    "            bar_position += 1\n",
    "\n",
    "            if metric == 'mode':\n",
    "                mode = value[0]\n",
    "                value = value[1]\n",
    "\n",
    "            if value is None or np.isnan(value):\n",
    "                value = -1\n",
    "\n",
    "            bar_plot = ax.bar(bar_position, value,\n",
    "                              label=metric, color=colors[metric])\n",
    "            autolabel(bar_plot, ax)\n",
    "\n",
    "        ax.set_xticks(range(len(metrics)))\n",
    "        ax.set_xticklabels(metrics, rotation=90, fontsize=15)\n",
    "\n",
    "        ax.set_title(\n",
    "            f'\\n Basic object metrics: {variable} \\n Mode: {mode}\\n',\n",
    "            fontsize=15)\n",
    "\n",
    "        fig.tight_layout()\n",
    "    return\n",
    "\n",
    "def get_numeric_stats(df):\n",
    "    \"\"\"\n",
    "    Esta magia sacará estadísticas básicas DE LAS VARIABLES NUMÉRICAS.\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    df: pandas.DataFrame\n",
    "        Tabla con variables limpias.\n",
    "\n",
    "    Regresa\n",
    "    -------\n",
    "    stats: diccionario\n",
    "        Dict de la forma {columna: {nombre de la métrica: valor de la métrica}}\n",
    "    \"\"\"\n",
    "    # Seleccionando las variables numéricas únicamente\n",
    "    numeric_df = df.select_dtypes(include=['float64', 'int64'])\n",
    "\n",
    "    # Este va a ser el diccionario que regresaremos, lo llenaremos con un looop.\n",
    "    stats = {}\n",
    "\n",
    "    # Recorramos las columnas\n",
    "    for numeric_column in numeric_df.columns:\n",
    "        # Obtengamos el promedio\n",
    "        mean = numeric_df[numeric_column].mean()\n",
    "\n",
    "        # Ahora la mediana\n",
    "        median = numeric_df[numeric_column].median()\n",
    "\n",
    "        # Ahora la desviación estándar\n",
    "        std = numeric_df[numeric_column].std()\n",
    "\n",
    "        # Obtengamos el primer y tercer cuartil\n",
    "        quantile25, quantile75 = numeric_df[numeric_column].quantile(\n",
    "            q=[0.25, 0.75])\n",
    "\n",
    "        # ¿Cuál es el porcentaje de nulos?\n",
    "        null_count = 100 * (\n",
    "        numeric_df[numeric_column].isnull().sum() / len(numeric_df))\n",
    "\n",
    "        # Guardemos\n",
    "        stats[numeric_column] = {'mean': mean,\n",
    "                                 'median': median,\n",
    "                                 'std': std,\n",
    "                                 'q25': quantile25,\n",
    "                                 'q75': quantile75,\n",
    "                                 'nulls': null_count\n",
    "                                 }\n",
    "    return stats\n",
    "\n",
    "def get_cat_stats(df):\n",
    "    \"\"\"\n",
    "    Esta magia sacará estadísticas básicas DE LAS VARIABLES CATEGÓRICAS\n",
    "\n",
    "    Parámetros\n",
    "    ----------\n",
    "    df: pandas.DataFrame\n",
    "        Tabla con variables limpias.\n",
    "\n",
    "    Regresa\n",
    "    -------\n",
    "    stats: diccionario\n",
    "        Dict de la forma {columna: {nombre de la métrica: valor de la métrica}}\n",
    "    \"\"\"\n",
    "    # Seleccionando los objetos\n",
    "    object_df = df.select_dtypes(include=['object', 'category'])\n",
    "\n",
    "    # El dict que regresaremos\n",
    "    stats = {}\n",
    "\n",
    "    # Recorramos las columnas\n",
    "    for object_column in object_df.columns:\n",
    "        # ¿Cuántos valores únicos hay?\n",
    "        unique_vals = len(object_df[object_column].unique())\n",
    "\n",
    "        # Saquemos la \"moda\" (valor más común).\n",
    "        # Para eso primero usamos value_counts para encontrar la frecuenc\n",
    "        all_values = object_df[object_column].value_counts()\n",
    "\n",
    "        # Ahora sacaremos una tupla con el valor más común y el porcentaje de veces\n",
    "        # que aparece\n",
    "        mode = (all_values.index[0],\n",
    "                100 * (all_values.values[0] / len(object_df)))\n",
    "\n",
    "        # Cuenta de nulos\n",
    "        null_count = (object_df[object_column].isnull().sum() / len(\n",
    "            object_df)) * 100\n",
    "\n",
    "        # Stats a devolver\n",
    "        stats[object_column] = {'unique_vals': unique_vals,\n",
    "                                'mode': mode,\n",
    "                                'null_count': null_count}\n",
    "\n",
    "    return stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "bf3832ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Seleccionar columnas que sean numéricas\n",
    "numeric_cols = df.select_dtypes(include=['int64', 'float64'])\n",
    "numeric_df = df[numeric_cols.columns]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d9d6ffb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables numéricas.\n",
    "numeric_stats = get_numeric_stats(numeric_df)   \n",
    "plot_numeric(df, numeric_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b626626",
   "metadata": {},
   "source": [
    "En las gráficas de estadísticas básicas, podemos ver lo siguiente:\n",
    "+ `dew`: No tiene datos atípicos y su distribución no está muy anormal\n",
    "+ `pollution`: Contiene varios datos atípicos, su promedio es de 94 y los valores suelen acotarse entre 24 y 132\n",
    "+ `press`: No hay datos atípicos y la distribución parece acercarse a la normal, los valores son muy similares entre sí\n",
    "+ `rain`: El promedio es de 0 y hay bastantes atípicos\n",
    "+ `snow`: Al igual que la variable de lluvia, su promedio es 0 y hay muchos atípicos, esto no debe alarmar ya que son las horas acumuladas de nieve/lluvia\n",
    "+ `temp`: No hay atípicos y su distribución parece ser normal, el promedio es de 14\n",
    "+ `wnd_spd`: Hay bastantes atípicos y la desviación estándar es muy alta, igual no hay que alarmarse ya que es la velocidad del viento acumulada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c945fb3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Variables categóricas.\n",
    "object_cols = df.select_dtypes(include=['object', 'category'])\n",
    "object_df = object_cols.loc[:, ~object_cols.columns.str.contains('date', case=False)]\n",
    "cat_stats = get_cat_stats(object_df)\n",
    "plot_categorical(df, cat_stats)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "25321538",
   "metadata": {},
   "source": [
    "Aquí podemos ver que la variable `wnd_dir` tiene 4 posibles valores y la moda es SE, indicando South-East."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "742a3db3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_codificado = df.copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "f48554e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def label_encode_variables_categoricas(df_codificado):\n",
    "    # Selecciona las columnas de tipo objeto (categóricas)\n",
    "    columnas_objeto = df_codificado.select_dtypes(include=['object']).columns\n",
    "\n",
    "    # Inicializa el codificador\n",
    "    label_encoder = LabelEncoder()\n",
    "\n",
    "    # Itera sobre las columnas seleccionadas y aplica el label encoding\n",
    "    for columna in columnas_objeto:\n",
    "        df_codificado[columna] = label_encoder.fit_transform(df_codificado[columna])\n",
    "\n",
    "    return df_codificado\n",
    "\n",
    "df_para_corr = label_encode_variables_categoricas(df_codificado)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb26d274",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcula la matriz de correlación excluyendo la columna date\n",
    "matriz_correlacion = df_para_corr.drop('date', axis=1).corr()\n",
    "\n",
    "# Configura el tamaño de la figura\n",
    "plt.figure(figsize=(8, 8))\n",
    "\n",
    "# Genera la matriz de correlación con Seaborn\n",
    "sns.heatmap(matriz_correlacion, annot=True, cmap='coolwarm', fmt=\".2f\")\n",
    "\n",
    "# Añade título\n",
    "plt.title('Matriz de Correlación')\n",
    "\n",
    "# Muestra la matriz de correlación\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d592204d",
   "metadata": {},
   "source": [
    "Matriz de correlaciones:\n",
    "1. Relaciones positivas fuertes\n",
    "    + `temp` y `dew` tienen una correlación positiva de 0.82\n",
    "2. Relaciones negativas fuertes\n",
    "    + `press` y `dew` tienen una correlación negativa de -0.78, `press` y `temp` de -0.83\n",
    "\n",
    "Las demás variables no muestran fuerte correlación con la siguiente más correlacionada siendo `dew` y `wnd_spd` con -0.30 "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2db8d1e8",
   "metadata": {},
   "source": [
    "#### Análisis univariado para SARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98f59ac9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert date column to datetime if it isn't already\n",
    "df['date'] = pd.to_datetime(df['date'])\n",
    "\n",
    "# Create time series with date as index and pollution as values\n",
    "time_series = df.set_index('date')['pollution']\n",
    "time_series = pd.DataFrame(time_series)\n",
    "time_series = time_series.rename(columns={'pollution': 'y'})\n",
    "time_series.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f9853aea",
   "metadata": {},
   "outputs": [],
   "source": [
    "time_series.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f054b4ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot the time series\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(time_series.index, time_series['y'])\n",
    "plt.title('Time Series of Pollution')\n",
    "plt.xlabel('Date')\n",
    "plt.ylabel('Pollution')\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f024ce6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vamos a graficar el boxplot\n",
    "time_series.boxplot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbbd0cfb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Detectar outliers con IQR\n",
    "Q1 = time_series['y'].quantile(0.25)\n",
    "Q3 = time_series['y'].quantile(0.75)\n",
    "IQR = Q3 - Q1\n",
    "lower_bound = Q1 - 1.5 * IQR\n",
    "upper_bound = Q3 + 1.5 * IQR\n",
    "outliers = time_series[(time_series['y'] < lower_bound) | (time_series['y'] > upper_bound)]\n",
    "print(\"Outliers detected:\")\n",
    "print(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "046fd861",
   "metadata": {},
   "outputs": [],
   "source": [
    "reporte(outliers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e6347bf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter time series for 2010-2011\n",
    "time_series_2010_2011 = time_series['2010-02':'2010-03']\n",
    "\n",
    "# Plot the filtered time series\n",
    "plt.figure(figsize=(15, 8))\n",
    "plt.plot(time_series_2010_2011.index, time_series_2010_2011['y'])\n",
    "plt.title('Time Series of Pollution February-March 2010')\n",
    "plt.xlabel('Date') \n",
    "plt.ylabel('Pollution')\n",
    "plt.grid()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e16bee5d",
   "metadata": {},
   "source": [
    "Al graficar y analizar la variable de interés, nos encontramos con varios datos atípicos, primero en el boxplot y después con el método del rango intercuartil. De los 43,800 registros que tenemos, nos marca 1,878 atípicos. Al ver una muestra de los que marca como atípicos, vemos valores superiores a 300. Al usar nuestra función de reporte, vemos que alcanzan máximos de hasta 994.\n",
    "\n",
    "Después de una breve investigación, encontramos que la página oficial de calidad del aire de Estados Unidos, AirNow, tiene los siguientes rangos de calidad:\n",
    "\n",
    "![Air Quality Table](images/Air_pollution_table.png)\n",
    "\n",
    "Con esta información, podría parecer que son errores de medición, sin embargo, valores de más de 300 pueden ser totalmentes posibles, ya que graficamos una parte en donde se encuentran estos valores atípicos, y parece que los días siguientes al atípico siguen siendo valores grandes, entonces no son ni errores de medición ni imposibles. Es por esto que decidimos dejarlos.\n",
    "\n",
    "AQI Basics | AirNow.gov. (n.d.). https://www.airnow.gov/aqi/aqi-basics/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1393f1e5",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dividir los datos en entrenamiento y prueba\n",
    "n_train = int(0.75 * len(time_series))\n",
    "train_data = time_series[:n_train]\n",
    "n_test = len(time_series) - n_train\n",
    "test_data = time_series[-n_test:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "ee9fe754",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf = LinearForecast(data=train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52aa7fb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf.adf_test() # Estacionariedad de los datos originales\n",
    "# Vamos a visualizar todas las transformaciones para luego decidir cuál será la elegida"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19c0b7c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf.timeseries_transformation(transformation=\"sqrt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1e5dd73",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf.timeseries_transformation(transformation=\"log\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0aab7dbe",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf.timeseries_transformation(transformation=\"yeo-johnson\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "120b8051",
   "metadata": {},
   "outputs": [],
   "source": [
    "train, lambda_value = lf.consolidated_ts_df(data=train_data, plot_graphs=True)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32367e8a",
   "metadata": {},
   "source": [
    "Después de ver los resultados de las transformaciones, elegiremos la de yeo-johnson. La segunda mejor fue la de log, sin embargo, como hay valores que son 0 en nuestra serie (misma razón por la cuál no aplicó box-cox), la de yeo-johnson es mejor."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "dda33d1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "train.drop([col for col in train.columns if 'yeo_j' not in col], axis=1, inplace=True)\n",
    "train.rename(columns={'yeo_j': 'y'}, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "6746ee07",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf = LinearForecast(data=train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b9f9dfc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf.plot_time_series()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "256e96f3",
   "metadata": {},
   "source": [
    "Ahora evaluaremos la estacionariedad con la prueba de ADF y la descomposición para analizar los componentes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ba2b609",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf.adf_test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "f305a9b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mstl_descomposition1(data, periods_seasonality, stl_kwargs=None):\n",
    "    \"\"\"\n",
    "    Perform MSTL decomposition to identify trend, seasonal, and residual components of a time series.\n",
    "\n",
    "    Parameters:\n",
    "        periods_seasonality (list): List of seasonal periods to be considered.\n",
    "        stl_kwargs (dict, optional): Additional keyword arguments for STL decomposition.\n",
    "\n",
    "    Returns:\n",
    "        DecomposeResult: A result object that includes trend, seasonal components, and residuals.\n",
    "    \"\"\"\n",
    "    if stl_kwargs is None:\n",
    "        stl_kwargs = {}\n",
    "\n",
    "    if data is None or data.empty:\n",
    "        raise ValueError(\"The data parameter is empty or None.\")\n",
    "\n",
    "    y = data['y']\n",
    "    mstl = MSTL(endog=y, periods=periods_seasonality, **stl_kwargs)\n",
    "    result = mstl.fit()\n",
    "\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8892f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "res = mstl_descomposition1(train,periods_seasonality=[24, 168]) # 24 por ser diario, 168 semanal\n",
    "res.plot().set_size_inches(12, 8)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "201b33dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Graficar datos observados con datos de estacionalidad\n",
    "ax = res.observed[:400].plot(label='observados')\n",
    "\n",
    "res.seasonal['seasonal_24'][:400].plot(ax=ax, label='seasonal 24')\n",
    "res.seasonal['seasonal_168'][:400].plot(ax=ax, label='seasonal 168')\n",
    "\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "560a737b",
   "metadata": {},
   "source": [
    "##### Estacionariedad de la estacionalidad encontrada"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "81bc763e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# En base a la estacionalidad seleccionada, calcular la estacionariedad de dicha serie temporal\n",
    "seasonality_24 = res.seasonal['seasonal_24']\n",
    "\n",
    "lf.adf_test(seasonality_24)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e04c4f94",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Estacionariedad de la segunda parte estacional\n",
    "seasonality_168 = res.seasonal['seasonal_168']\n",
    "\n",
    "lf.adf_test(seasonality_168)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "0b910557",
   "metadata": {},
   "outputs": [],
   "source": [
    "trend_data = res.trend\n",
    "resid_data = res.resid"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "265817fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf.adf_test(trend_data) # Estacionariedad de la tendencia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfff8349",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf.adf_test(resid_data) # Estacionariedad de residuos"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e097f3a",
   "metadata": {},
   "source": [
    "##### Genere los correlogramas ACF y PACF para evaluar patrones de autocorrelación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c70bf72",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf.plot_acf_pacf()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "366c0ed1",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf.plot_acf_pacf(data=train.diff().diff(24).dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "68aa41e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf.plot_acf_pacf(data=train.diff().diff(168).dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47acdf87",
   "metadata": {},
   "source": [
    "Basado en las gráficas, los posibles valores son:\n",
    "\n",
    "P -> 1\n",
    "\n",
    "D -> 1\n",
    "\n",
    "Q -> 1\n",
    "\n",
    "Ahora para el componenete no estacional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d0654c5f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la estacionariedad de mis datos observados\n",
    "lf.adf_test(res.observed.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b4bcaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular la componente no estacional de la serie temporal basado en la S anterior\n",
    "non_seasonal = res.observed - res.seasonal['seasonal_24'] - res.seasonal['seasonal_168'] \n",
    "\n",
    "res.observed[:200].plot()\n",
    "non_seasonal[:200].plot()\n",
    "plt.legend(['observed', 'non-seasonal'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be4fdce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calcular si la serie no estacional es estacionaria o no\n",
    "lf.adf_test(non_seasonal.dropna())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38d1ae94",
   "metadata": {},
   "outputs": [],
   "source": [
    "lf.plot_acf_pacf(non_seasonal.dropna())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ed11cd0",
   "metadata": {},
   "source": [
    "Para los posibles valores son:\n",
    "\n",
    "p -> 2\n",
    "\n",
    "d -> 0\n",
    "\n",
    "q -> 0"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84f08403",
   "metadata": {},
   "source": [
    "### SARIMAX"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "40200f4a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import PowerTransformer\n",
    "\n",
    "pt = PowerTransformer(method='yeo-johnson')\n",
    "# Transformar los datos de entrenamiento y prueba\n",
    "train_transformed = pt.fit_transform(train_data.values.reshape(-1, 1))\n",
    "test_transformed = pt.transform(test_data.values.reshape(-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1be8615c",
   "metadata": {},
   "source": [
    "Vamos a loggear el experimento en MLFlow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14645012",
   "metadata": {},
   "outputs": [],
   "source": [
    "dagshub.init(url=\"https://dagshub.com/daduke1/proyecto-modelos\", mlflow=True)\n",
    "\n",
    "MLFLOW_TRACKING_URI = mlflow.get_tracking_uri()\n",
    "\n",
    "print(MLFLOW_TRACKING_URI)\n",
    "\n",
    "mlflow.set_tracking_uri(MLFLOW_TRACKING_URI)\n",
    "mlflow.set_experiment(experiment_name=\"proyecto-modelos\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062ad9cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "order = (2, 0, 0)\n",
    "seasonal_order = (1, 1, 1, 24)\n",
    "\n",
    "with mlflow.start_run() as run:\n",
    "    # Log parameters\n",
    "    mlflow.log_param(\"order\", order)\n",
    "    mlflow.log_param(\"seasonal_order\", seasonal_order)\n",
    "\n",
    "    # Fit model\n",
    "    model = SARIMAX(train_transformed, order=order, seasonal_order=seasonal_order,\n",
    "                    enforce_stationarity=False, enforce_invertibility=False)\n",
    "    results = model.fit(disp=False)\n",
    "\n",
    "    # Save model\n",
    "    model_file = \"sarimax_model.pkl\"\n",
    "    joblib.dump(results, model_file)\n",
    "    mlflow.log_artifact(model_file)\n",
    "\n",
    "    # Forecast\n",
    "    forecast_trans = ph_model.forecast(steps=len(test_transformed))\n",
    "    forecast_trans = pd.DataFrame(forecast_trans)\n",
    "    forecast_inv = pt.inverse_transform(forecast_trans).flatten()\n",
    "\n",
    "    # Compute and log metrics\n",
    "    rmse = np.sqrt(mean_squared_error(test_data, forecast_inv))\n",
    "    mse = mean_squared_error(test_data, forecast_inv)\n",
    "    mape = mean_absolute_percentage_error(test_data, forecast_inv)\n",
    "    r2 = r2_score(test_data, forecast_inv)\n",
    "\n",
    "    mlflow.log_metric(\"RMSE_test\", rmse)\n",
    "    mlflow.log_metric(\"MSE_test\", mse)\n",
    "    mlflow.log_metric(\"MAPE_test\", mape)\n",
    "    mlflow.log_metric(\"R2_test\", r2)\n",
    "\n",
    "    # Log model summary\n",
    "    summary_file = \"model_summary.txt\"\n",
    "    with open(summary_file, \"w\") as f:\n",
    "        f.write(results.summary().as_text())\n",
    "    mlflow.log_artifact(summary_file)\n",
    "\n",
    "    # Plot forecast vs actual\n",
    "    plt.figure(figsize=(10, 6))\n",
    "    plt.plot(test_data.index, test_inv, label=\"Actual\")\n",
    "    plt.plot(test_data.index, forecast_inv, label=\"Forecast\", linestyle='--')\n",
    "    plt.title(\"SARIMAX Forecast vs Actual (Test Set)\")\n",
    "    plt.legend()\n",
    "    plt.tight_layout()\n",
    "    plot_file = \"forecast_vs_actual.png\"\n",
    "    plt.savefig(plot_file)\n",
    "    plt.close()\n",
    "    mlflow.log_artifact(plot_file)\n",
    "\n",
    "    print(f\"Logged SARIMAX model. RMSE: {rmse:.2f}, MAPE: {mape:.2%}, R²: {r2:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca0ddae6",
   "metadata": {},
   "source": [
    "El modelo SARIMAX elegido dió buenos resultados, con las siguientes métricas:\n",
    "\n",
    "+ MSE: 0.11\n",
    "+ MAPE: 77.79%\n",
    "+ R2: 0.894"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.undefined"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
